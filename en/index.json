[
{
	"uri": "/en/intro.html",
	"title": "Introduction",
	"tags": [],
	"description": "",
	"content": " Through this workshop, you can easily configure SAP HANA High Availability Architecture using SAP HANA Quick Starts, and learn best practices for monitoring and maintenance for HANA DB cluster operation, redundancy testing, and changing cluster settings.\n  Preparation  AWS Account : You need authorities to create and access CloudFormation, EC2, VPC, S3, IAM, Systems Manager resources. AWS Region : This lab uses N.Virginia (us-east-1) region. Prior knowledge of AWS and SAP on AWS is required. (Gereral and SAP Immersionday required) Web Browser : Please use the latest version of Chrome or Firefox.    Contact Us If you have any questions about AWS services, please contact AWS Support or your AM, and if you have any questions regarding the presentation materials of this workshop, please contact us through the email link below.\n SangGyu Lee, SAP Partner Solutions Architect : leesangg@amazon.com JinUk Lee, SAP Solutions Architect : jinuklee@amazon.com Sejun Kim, Solutions Architect : sejun@amazon.com Byungsoo Kang, SAP Partner Dev Manager : byungsk@amazon.com   © 2020, Amazon Web Services, Inc. or its affiliates. All rights reserved.\r"
},
{
	"uri": "/en/",
	"title": "Home",
	"tags": [],
	"description": "",
	"content": "SAP HANA on AWS Advanced WorkShop  This program aims to provide lectures for hands-on advanced practice of SAP HANA on AWS.\r"
},
{
	"uri": "/en/lab1.html",
	"title": "Lab 01. Setup Cluster",
	"tags": [],
	"description": "",
	"content": "Lab Introduction Install SAP HANA Cluster for using throughout the lab.\n  Lab Architecture  © 2020, Amazon Web Services, Inc. or its affiliates. All rights reserved.\r"
},
{
	"uri": "/en/lab1/lab1-1.html",
	"title": "Task 01. SAP HANA Quick Starts",
	"tags": [],
	"description": "",
	"content": " Easily configure SAP HANA High Availability Architecture using SAP HANA Quick Starts.\n  Considerations  The OS of each node in this lab will be configured with SUSE Linux Enterprise 12 SP4. SAP HANA Quick Starts are set to Performance Optimized Scenario. For more information, please refer to the link below.  SAP HANA High Availability Cluster for the AWS Cloud - Setup Guide       Pre-work : Create Key Pairs Create a Key Pair to access HANA Database and Application Servers.\n After log in AWS Management Console, connect to Key Pairs Console. Click Create key pair.  Enter Name as SAP-ImmersionDay-Lab, Click Create key pair.  In this WorkShop, you will access HANA DB instances through AWS Systems Manager Session Manager, and in the case of Bastion Host (Windows Server 2019), you will decrypt Administrator Password using this key pair and access it through a remote access program. Store the downloaded key in a safe location.  You are going to create and use a Bastion Host in Lab02     SAP HANA Quick Starts There are many options for installation in SAP HANA Quick Starts. In this Lab, you configure HANA Database in a new VPC based on Multi-AZ. For more information on SAP HANA Quick Starts, please refer to the link below.(SAP HANA Quick Start Guide)\n  After log in to AWS Management Console, access SAP HANA Quick Starts service.\n  Make sure the region is US East(N.Virginia), then click Next.   Don\u0026rsquo;t change the stack name unless it\u0026rsquo;s the same name you used previously.\n  Step2.Specify stack details is the step to enter the VPC, SAP HANA, and SAP S/4HANA ABAB Cluster setup options for S4HANA installation.\n  Enter Step2.1 Network infrastructure configuration option as follows.\n VPC CIDR(default) : 10.0.0.0/16 Availability Zones for subnet creation(choice) : us-east-1a, us-east-1b CIDR block for Private subnet 1(default) : 10.0.1.0/24 CIDR block for Private subnet 2(default) : 10.0.2.0/24 Enter CIDR block for Public subnet 1(default) : 10.0.3.0/24 Enter CIDR block for Public subnet 2(default) : 10.0.4.0/24     Enter Step2.2 Server and storage configuration option as follows.\n Choose operating system for SAP HANA : SuSELinux12SP4ForSAP-BYOS Enter SUSE BYOS Registration Code : XXXXXXXXXXXXXXXX (Enter your registration code) Choose instance type for SAP HANA : r5.4xlarge Enter Dedicated Host ID : Choose key pair : SAP-ImmersionDay-Lab Choose storage volume type for SAP HANA Log : gp2 Choose storage volume type for SAP HANA Data : gp2 Would you like to turn on encryption? : no     Enter Step2.3 SAP HANA database configuration option as follows.\n Enter domain name : local Enter SAP HANA Primary host name : prihana Enter SAP HANA Secondary host name : sechana Enter SAP HANA system ID : HDB Enter SAP HANA instance number : 00 Enter SAP HANA password : Init12345! Enter SAP HANA Server timezone : UC Enter Amazon S3 URL for SAP HANA software : (e.g s3://sap-immsersionday-hol1/hanadb/) Install SAP HANA software? : Yes     Step2.4 SAP HANA High Availability configuration option, use the default setting\n HANA Primary Site Name : HAP HANA Secondary Site Name : HAS Overlay IP Address : 192.168.1.99 Pacemaker Tag Name : PaceTag     Step2.5 Optional configuration RDP and Bastion settings will be covered in Lab02, so select No without change. Skip the rest of the options.   Step2.6 Advanced configuration (Do not modify unless directed by AWS Support) option, use the default setting. click Next.   Step3.Configure stack options is the step to set Stack execution options.\n  When errors occur, change the Rollback option to Disabled for easy trouble shooting. then select Next.   Step4.Review is a step to check the previous settings. Select the two check boxes at the bottom of Capabilities and click Create stack.   SAP-HANA-HA has been created. Wait until the Status changes CREATE_COMPLETE.   If errors occur, contact the Solutions Architect\n  Lab01 lab has completed. Next, proceed to Lab02.\n  © 2020, Amazon Web Services, Inc. or its affiliates. All rights reserved.\r"
},
{
	"uri": "/en/lab2.html",
	"title": "Lab 02. Monitor Cluster",
	"tags": [],
	"description": "",
	"content": "Lab Introduction Let’s see how to monitor the SAP HANA Cluster configured in Lab01 and how to access Overlay IP from OnPrem and other VPCs using Transit Gateway.\n  Lab Architecture  © 2020, Amazon Web Services, Inc. or its affiliates. All rights reserved.\r"
},
{
	"uri": "/en/lab4/lab4-1.html",
	"title": "Task 01. Migrate Cluster",
	"tags": [],
	"description": "",
	"content": " Task 01 shows how to perform a manual migration from SAP HANA running on the master node to the secondary (standby) node using the crm CLI. Before starting this lab, you should first verify that all services in the cluster are running normally without errors.\n  Architecture  Migrating HANA primary   Lab 02. Task 03. Execute HAWK (High Availability Web Konsole) to check the cluster status through Web.\n  Execute Lab 03. Task 01. Validation of Cluster to check the cluster status through CLI.\n  After logging in to the Management Console to migrate the Overlay IP resource, connect to EC2 Instance Console\n  Select HANA-HDB-Primary instance, click Action, and click Connect.   Select Session Manager and click Connect. Connect to prihana instance through Session Manager.   On Node1 (prihana), run the command below to migrate the Overray IP resource res_AWS_IP to Node2 (sechana).\nsudo su - crm resource migrate res_AWS_IP force   Since you used the \u0026ldquo;migrate\u0026rdquo; command, the cluster stops the RA of the Primary Node (prihana) in its current state and promotes the Secondary Node to \u0026ldquo;Master\u0026rdquo;. Primary should not be migrated if system clone is in INSYNC (SFAIL) state. Wait for the Secondary Node to become the new Primary.\n  Connect to sechana and check the cluster and HSR status.\n  Connect to EC2 Instance Console\n  Select HANA-HDB-Secondary instance, click Action, and click Connect.   Select Session Manager and click Connect. Connect to sechana instance through Session Manager   Check the cluster status.\n crm_mon is a command that provides the current state of the cluster (using root user). Execute the command below to check if sechana is in Master status.  sudo su - crm_mon -rfn1   Check SAP HANA System Replication (HSR) status.\n Use SAPHanaSR-showAttr to confirm that the prihana node is SOK. (Use root user)  sudo su - SAPHanaSR-showAttr   Connect to Bastion Host and check the Dashboard of HAWK web. (Refer to Lab02)\n You can see that Overlay IP Resource has been moved to sechana.     On Secondary Node (sechana), disable resource migration so that the cluster can start on Primary Node (prihana).\nsudo su - crm resource unmigrate res_AWS_IP    © 2020, Amazon Web Services, Inc. or its affiliates. All rights reserved.\r"
},
{
	"uri": "/en/lab2/lab2-1.html",
	"title": "Task 01. Verify Created Resources",
	"tags": [],
	"description": "",
	"content": " Check out the important resources created with SAP HANA Quick Starts.\n   Check created SAP HANA Instance. After log in to AWS Management Console, access EC2 Console service.\n  You can see HANA DB Primary and Secondary instances have been created.   Select HANA DB Primary or Secondary instance and look at Description below to see that Secondary private ip has been created. This Secondary IP is used to communicate between two nodes (used to check if other nodes have issues).   Select HANA DB Primary or Secondary instance and look at Tags tab below to see that PaceTag(entered as an option in Quick Starts) has been created. Since hostname of EC2 Instance is automatically created, Cluster agent identifies HANA DB Instance by looking at this tag.   If you select HANA DB Primary or Secondary instance and click the Connect at the top, you can connect to the instance through Session Manager. click Connect to connect to the instance. In Lab03, we plan to conduct HA test by connecting to the instance through Session Manager.   Check created VPC and Subnets.. After log in to AWS Management Console, access Subnets Console service.\n  You can see that two private and two public subnets were created each.   Check Route Tables menu on the left. You can check the routing for Public and Private Subnet.\n  If you select Private subnet route table and check Routes tab below, you can see the routing entry for Overlay IP(see Task 02. Connect to Overlay IP for more information). If you click eni defined in Target, you can check that it is currently connected to Primary DB. (* Note: Private IP is different for each user.)\n In Lab03, please check how Route Table changes after being taken over to Secondary DB.      © 2020, Amazon Web Services, Inc. or its affiliates. All rights reserved.\r"
},
{
	"uri": "/en/lab2/lab2-2.html",
	"title": "Task 02. Connect to Overlay IP",
	"tags": [],
	"description": "",
	"content": " When configuring High Availability Cluster in Multi-AZ environment, VIP used by Application Server and HANA Database use Overlay IP outside of VPC CIDR. Transit Gateway can be used to access this in other VPCs or OnPremise environment. In this Lab, you are going to create a Custom VPC through CloudFormation, set up a transit gateway, and connect to HANA Database\u0026rsquo;s Overlay IP through Bastion Host.\n This task consists of 5 steps.\n Create a Custom VPC and a Bastion Host using CloudFormation Create and configure a Transit Gateway Update route table of subnet with HANA Instances Update Security Group for HANA Instance Test Overlay IP connection at Bastion Host   Create a Custom VPC and a Bastion Host using CloudFormation  After log in to AWS Management Console, Connect to CloudFormation for Customer VPC If Network Configuration setting does not overlap with the existing one, the default setting is used.  Check two checkboxes of Capabilities at the bottom of the screen and click Create stack.  CustomVPC stack has been created. Wait until the Status changes CREATE_COMPLTE.  Connect to EC2 Instance Console. You can see that Bastion Host (Windows Sever 2019) was created on Custom VPC-Public Subnet.    Create and configure a Transit Gateway Create a transit gateway to associate SAP HANA VPC and Custom VPC.\n  Connect to Transit Gateways Console, create a transit gateway, and associate SAP HANA VPC and Custom VPC. then register the entry for connecting the Overlay IP to Routing Table on Transit Gateway.\n  Click Create Transit Gateway.   Enter HANA-TGW in Name Tag, Click Create Transit Gateway. then click Close.   Wait until the State of HANA-TGW you just created changes available.   To attach SAP HANA VPC and Custom VPC, Connect to Transit Gateways Attachment Console. Click Create Transit Gateway Attachment.   Select created HANA-TGW, and enter HANA VPC as Attachment name tag. Choose a VPC ID that starts with SAP-HANA.   Select Subnet ID as Private Subnet 1,2 and click the Create attachment at the bottom right. Click the Close.   Click Create Transit Gateway Attachment again.   Similarly, select HANA-TGW, and enter Custom VPC as Attachment name tag. Select Custom as VPC ID.   Select Subnet ID as Custom Public Subnet AZ1, AZ2, and click Create attachment at the bottom right. Click Close button   Wait for both attachments to change State as available (After a certain period of time, click the Refresh button to check the State.)   Connect to Transit Gateways Route Tables Console and register the routing for Overlay IP.\n  Select created route table and select Routes tab, then click Create static route   For CIDR option, enter 192.168.1.99/32.\n  For Choose attachment option, choose HANA VPC and click Create static route.   Click Close.\n  You can check that static route for Overlay IP is registered.    Update Instance route table Updated each route table so that HANA DB Instance and Bastion Host can communicate with each other through Transit Gateway.\n Connect to Route Tables Console Firstly, update the route table of HANA DB Instance. Choose Private subnet route table and select Routes tab. then click Edit routes.  Click Add routes and enter Destination as 10.1.0.0/16, then choose Target as Transit Gateway. Choose HANA-TGW and click Save routes.  Click Close. Next, update the route table of Bastion Host. Choose Custom Public Routes and select Routes tab, then click Edit routes.  Click Add routes and enter Destination as 10.0.0.0/16, then choose Target as Transit Gateway. Choose HANA-TGW Click Add routes again, enter Destination as 192.168.1.99/32, then choose Target as Transit Gateway. Choose HANA-TGW Confirm both routes are registered and click Save routes.  Click Close.   Update Instance Security Group Allows inbound traffic for Bastion Host to the Security Group of Primary and Secondary HANA DB Instances.\n First, check Bastion Host IP. Connect to EC2 Instance Console. Check the Private IP of Custom Bastion Host (e.g 10.1.3.154)  Update Primary instance. Choose HANA-HDB-Primary instance and click Security Groups link below.  Select Inbound rules tab at the bottom and click Edit inbound rules.  Click Add rule at the bottom, select All traffic as Type, and enter the CIDR of Bastion Host found above(e.g 10.1.3.154/32)  Because it is a lab environment, Bastion Host is allowed with All traffic. It is recommended to allow only the necessary ports in a real environment.    Click Save rules at the bottom. Same as Primary Instance, update Security Group for Secondary Instance. Connect to EC2 Instance Console. Choose HANA-HDB-Secondary instance and click the Security Groups link.  Select Inbound rules tab at the bottom and click Edit inbound rules. Click Add rule at the bottom, select All traffic as Type, and enter the CIDR of Bastion Host found above(e.g 10.1.3.154/32)  Because it is a lab environment, Bastion Host is allowed with All traffic. It is recommended to allow only the necessary ports in a real environment.    Click Save rules at the bottom.   Connect to Bastion Host and Test Overlay IP Connect to Bastion Host and perform a ping test to see if it is possible to connect to the Overlay IP.\n Connect to EC2 Instance Console and check the Public IP of Custom Bastion Host. (e.g 35.173.137.37)  Choose Custom Bastion Host and click Connect.  Click Get Password.  Click Select File and select SAP-ImmersionDay-Lab.pem file saved in Lab01.  The screen below may look different depending on the OS. The screenshot below is a MacBook screen.    Click Decrypt Password to confirm Administrator Password.(e.g qRPIUIXXSI5ceUhLuntol%WhZ\u0026amp;WFzCT$)  Click Close. Connect to Bastion Host using a remote access program.  Refer to remote access method in Windows OS Refer to remote access method in MAC OS   When connected successfully, search for Windows PowerShell in the Search window and run it.  When PowerShell is launched, execute ping 192.168.1.99 to check if communication with Overlay IP is possible.   Shut down the PowerShell and proceed with Task03 while maintaining the remote connection.\n  © 2020, Amazon Web Services, Inc. or its affiliates. All rights reserved.\r"
},
{
	"uri": "/en/lab4/lab4-2.html",
	"title": "Task 02. Take a node offline",
	"tags": [],
	"description": "",
	"content": " In this lab, we will show you how to put a cluster node on standby so that all cluster services running on the cluster node are automatically migrated to the secondary node.\n  Architecture  Taking a node offline   Connect to Bastion Host and check the Dashboard of HAWK web (refer to Lab02)\n OYou can see that Overlay IP Resource has been moved to sechana.     Connect to sechana and check the cluster and HSR status.\n  Connec to EC2 Instance Console\n  Select HANA-HDB-Secondary instance, click Action, and click Connect.   Select Session Manager and click Connect. Connect to sechana instance through Session Manager   Check the cluster status.\n crm_mon is a command that provides the current state of the cluster (using root user). Execute the command below to check if sechana is in Master status.  sudo su - crm_mon -rfn1   Check SAP HANA System Replication (HSR) status.\n Use SAPHanaSR-showAttr to confirm that the prihana node is SOK. (Use root user)  sudo su - SAPHanaSR-showAttr   Run the command below to put Node2 (sechana) into standby mode. (Use root user)\nsudo su - crm node standby sechana   The cluster starts prihana SAP HANA database, and takeover from the sechana. If the system replication synchronization was performed. Wait for the prihana to become brand new Primary.\n  Connect to Bastion Host and check the Dashboard of HAWK web (refer to Lab02)\n You can see that Overlay IP Resource has been moved to prihana.     Check the cluster status until the resource is migrated from Node2 (sechana) to Node1 (prihana).\n crm_mon is a command that provides the current state of the cluster (using root user). Execute the command below to check if prihana is in Master status. Confirm that the sechana node is in the standby state.  sudo su - crm_mon -rfn1   Set Cluster Node2 (sechana) online (using root user)\nsudo su - crm node online sechana   Cluster status is checked until Cluster Node2 (sechana) is online.\n crm_mon is a command that provides the current state of the cluster (using root user). Execute the command below to check if sechana is in online status.  sudo su - crm_mon -rfn1   Cleans up the SAPHana resource status.\ncrm resource cleanup rsc_SAPHana_HDB_HDB00    © 2020, Amazon Web Services, Inc. or its affiliates. All rights reserved.\r"
},
{
	"uri": "/en/lab2/lab2-3.html",
	"title": "Task 03. HAWK(High Availability Web Konsole)",
	"tags": [],
	"description": "",
	"content": " You can access the HAWK (High Availability Web Konsole) and check the HANA DB cluster status on the web screen.\n   Connect to Bastion Host (Windows Server 2019) using a remote access program.  Refer to Task02 for how to check IP and Administrator password. Refer to remote access method in Windows OS Refer to remote access method in MAC OS   Press the Windows button and connect to Server Manager.  Select Local Server to disable Internet Explorer\u0026rsquo;s security settings, and select On in IE Enhanced Security Configuration.  Change both Administrator and Users to Off and select the OK button.  Close Server Manager. Then launch Internet Explorer.  Enter \u0026ldquo;https://192.168.1.99:7630\u0026rdquo; in the address bar. (IP: Overlay IP, Port: 7630) Click More information and click Go on to the webpage.  The HAWK Login page is loaded. Enter ID: hacluster , Password: Init12345! (The password was entered as SAP HANA password in Quick Starts option of Lab01.). Click Login.  Select the Dashboad menu. You can check the current status of each node and resource status by node.   Proceed with Lab03 while maintaining remote access. You are going to check the dashboard status during HA test.\n  © 2020, Amazon Web Services, Inc. or its affiliates. All rights reserved.\r"
},
{
	"uri": "/en/lab4/lab4-3.html",
	"title": "Task 03. Perform SAP HANA System Replica",
	"tags": [],
	"description": "",
	"content": " In some scenarios where a cluster TakeOver or failover occurs quickly, or a node rejoins the cluster before the TakeOver is complete, the SAP HANA system replication may become inconsistent, requiring manual forced synchronization of the entire system replica.\n Set Cluster Maintenance Mode Put the cluster in maintenance mode on the primary node prihana\n  After log in Management Console, connect to EC2 Instance Console\n  Select HANA-HDB-Primary instance, click Action, and click Connect.\n  Select Session Manager and click Connect.\n  Connect to prihana instance through Session Manager.   Putting the cluster into maintenance mode (using root user)\n To perform system replication synchronization of HANA Database, set the cluster to Maintenance Mode by executing the command below.  sudo su - crm configure property maintenance-mode=true   Check the cluster status.\n crm_mon is a command that provides the current state of the cluster (using root user). Check if Cluster is unmanaged by executing the command below.  sudo su - crm_mon -rfn1   Log in as hdbadm and check the replication status.\nsu – hdbadm HDBSettings.sh systemReplicationStatus.py exit   SAP HANA system replica SAP HANA system replication manual execution in the secondary node sechana\n Connect to EC2 Instance Console\n  Select HANA-HDB-Secondary instance and click Connect.   Select Session Manager and click Connect. Connect to sechana instance through Session Manager.   Log in as hdbadm and stop Secondary SAP HANA Database. (using hdbadm user)\nsu – hdbadm HDB stop exit   Execute system replication with the following command (using hdbadm user)\nsu – hdbadm hdbnsutil -sr_register --name=HAS --remoteHost=prihana --remoteInstance=00 --replicationMode=sync --force_full_replica exit   Start Secondary SAP HANA Database. (using hdbadm user)\nsu – hdbadm HDB start exit   Disable Cluster Maintenance Mode Check the status of the cluster on the primary node, prihana, and then release maintenance mode.\n You can check the full replica status on the Primary Node. (using hdbadm user)\nsudo su - hdbadm watch -n 1 HDBSettings.sh systemReplicationStatus.py exit   Since SAP HANA system replication operation is complete, take the cluster out of maintenance mode. (Use root user)\nsudo su - crm configure property maintenance-mode=false   Check the cluster status.\n crm_mon is a command that provides the current state of the cluster (using root user). Execute the command below to check if Cluster has changed to the normal operating state.  sudo su - crm_mon -rfn1    © 2020, Amazon Web Services, Inc. or its affiliates. All rights reserved.\r"
},
{
	"uri": "/en/lab4/lab4-4.html",
	"title": "Task 04. Test Resource Agent",
	"tags": [],
	"description": "",
	"content": " Cluster resource agent testing is an important part of cluster management to ensure correct cluster configuration, troubleshoot, and investigate. Resource agents should not be tested with a running cluster or application as they can cause service outages on the node. Make sure the cluster is stopped on both nodes before starting this test.\n  STOP HA Cluster After stopping secondary node(sechana), stop primary node(prihana)\n  Connect to EC2 Instance Console\n  Select HANA-HDB-Secondary instance and click Connect.   Select Session Manager and click Connect. Connect to sechana instance through Session Manager.   verify the status of sechana cluster (using root user)\nsudo su - crm cluster status   stop sechana Cluster (using root user)\nsudo su - crm cluster stop   verify the status of sechana cluster (using root user)\nsudo su - crm cluster status   Connect to EC2 Instance Console\n  Select HANA-HDB-Primary instance, click Action, and click Connect.\n  Select Session Manager and click Connect. Connect to prihana instance through Session Manager.   verify the status of prihana cluster (using root user)\nsudo su - crm cluster status   stop prihana Cluster (using root user)\nsudo su - crm cluster stop   verify the status of prihana cluster (using root user)\nsudo su - crm cluster status    Test EC2 Stonith agent The command to test the EC2 Stonith agent is as follows. Change the parameters to suit your situation and run. (When testing EC2 Stonith agent, one of the nodes is stopped to verify that IAM policy, AWS CLI configuration, and connection to EC2 API endpoint are working properly.)\nstonith -t external/ec2 profile={AWS-PROFILE} port={CLUSTER-NODE2} tag={AWS-TAG-CONTAINING-HOSTNAME} -T off {CLUSTER-NODE2} The parameters can be checked with the command below. (using root user)\nsudo su - crm configure show res_AWS_STONITH  {AWS-PROFILE} : cluster : Use the default and, if necessary, replace it with a different profile name. {CLUSTER-NODE} : sechana or prihana : Name or IP address of another cluster node. {AWS-TAG-CONTAINING-HOSTNAME} : PaceTag : The tag name of the EC2 instance for both cluster nodes.    (prihana) The command below stop sechana. (using root user)\nsudo su - stonith -t external/ec2 profile=cluster port=sechana tag=PaceTag -T off sechana   Connect to EC2 Instance Console, verify the status of sechana.\n  HANA-HDB-Secondary instance is in stopped state. Start sechana to check if it is in a normal state.   Select HANA-HDB-Secondary instance, and click Action button. Press Start in Instance State. Press Yes, Start button.   When HANA-HDB-Secondary boots normally, connect to prihana instance\n  (prihana) You can only check the status of a node with the following command. (using root user)\nsudo su - stonith -t external/ec2 profile=cluster port=sechana tag=PaceTag -S    Test Overlay IP Agent (aws-vpc-move-ip) When testing the overlay IP agent, the overlay IP address of the node on which the test runs is configured locally and the VPC route table is modified accordingly. On both systems, run the following command as root user, one system at a time\nOCF_RESKEY_address={VIRTUAL-IPV4-ADDRESS} \\ OCF_RESKEY_routing_table={AWS-ROUTE-TABLE} \\ OCF_RESKEY_interface=eth0 OCF_RESKEY_profile={AWS-PROFILE} \\ OCF_ROOT=/usr/lib/ocf /usr/lib/ocf/resource.d/suse/aws-vpc-move-ip start The parameters can be checked with the command below. (using root user)\nsudo su - crm configure show res_AWS_IP  {VIRTUAL-IPV4-ADDRESS} : 192.168.1.99 : VIP of HANA DB Cluster {AWS-ROUTE-TABLE} : The AWS route table ID for the overlay IP address to be reached from prihana host. You can get this by navigating to SAP-HANA-HA* VPC, then to Private subnet 1, then to Route table. {AWS-PROFILE} : cluster : Use the default and, if necessary, replace it with a different profile name.    Start overlay IP agent (using root user)\n {AWS-ROUTE-TABLE} : Need to change to confirmed route table ID  sudo su - OCF_RESKEY_address=192.168.1.99 \\ OCF_RESKEY_routing_table=\u0026lt;AWS-ROUTE-TABLE\u0026gt; \\ OCF_RESKEY_interface=eth0 OCF_ROOT=/usr/lib/ocf \\ OCF_RESKEY_profile=cluster /usr/lib/ocf/resource.d/suse/aws-vpc-move-ip start   You can also check the IP configured in Linux with the following command (using root user)\nip address show   Check the IP on a node that is not being tested. (e.g if you are currently testing overlay IPs on prihana, check with sechana)\n  Stop overlay IP agent (using root user)\n {AWS-ROUTE-TABLE} : Need to change to confirmed route table ID  sudo su - OCF_RESKEY_address=192.168.1.99 \\ OCF_RESKEY_routing_table=\u0026lt;AWS-ROUTE-TABLE\u0026gt; \\ OCF_RESKEY_interface=eth0 OCF_ROOT=/usr/lib/ocf \\ OCF_RESKEY_profile=cluster /usr/lib/ocf/resource.d/suse/aws-vpc-move-ip stop    Start HA Cluster Now that the test is complete, start HA Cluster. Start Primary node cluster (prihana) and then Secondary node (sechana) cluster\n  Connect to EC2 Instance Console\n  Select HANA-HDB-Primary instance, click Action, and click Connect.\n  Select Session Manager and click Connect. Connect to prihana instance through Session Manager.   Start Primary node(prihana) cluster (using root user)\nsudo su - crm cluster start   Verify Primary node(prihana) Cluster (using root user)\nsudo su - crm cluster status   Connect to EC2 Instance Console\n  Select HANA-HDB-Secondary instance and click Connect.   Select Session Manager and click Connect. Connect to sechana instance through Session Manager.   Start Secondary node(sechana) Cluster (using root user)\nsudo su - crm cluster start   Verify Secondary node(sechana) Cluster\nsudo su - crm cluster status    © 2020, Amazon Web Services, Inc. or its affiliates. All rights reserved.\r"
},
{
	"uri": "/en/lab4/lab4-5.html",
	"title": "Task 05 - Upgrade Cluster Software(Optional)",
	"tags": [],
	"description": "",
	"content": " Depending on the update status at the time of the lab, the update may not be applied or the state of the currently configured HA cluster may affect unexpected test scenarios. If you need to troubleshoot an unexpected impact, it will be outside the scope of this workshop and will affect the progress of the Lab 06. Change to Cost Optimized Scenario that will be conducted later. We recommend that you do it for the last time after all labs are over.\n In this lab we will show you how to update SUSE Linux on both cluster nodes.\n  Update Cluster Software   Stop or Maintenance Cluster\n If a package update requires a reboot, stop the cluster stack on the secondary node before starting the software update.  sudo su - crm cluster stop  If not needed, switch to maintenance mode  crm node maintenance {NODE_NAME}   Install package updates :\nsudo su - zypper up   Show available patches :\nsudo su - zypper lp   Show available updates :\nsudo su - zypper lu   Start or Off Maintenance Cluster\n Start the cluster on each node or restart the server.  sudo su - crm cluster start or sudo su - reboot  Off Maintenance Cluster  crm node maintenance {NODE_NAME} off   Lab 03. Execute Task 01. Validation of Cluster to check the cluster status through CLI.\n  Lab 02. Task 03. Execute HAWK (High Availability Web Konsole) to check the cluster status through the web.\n   Performing Cluster-wide Rolling Upgrade If you are performing rolling upgrade of the entire cluster, you need to take one node offline to proceed with the upgrade, then take back and repeat the upgrade procedure on the other node.\n  Log in as root on the node you want to upgrade and stop the cluster.  crm cluster stop Perform the upgrade to the desired target version of SUSE Linux Enterprise Server for SAP applications Start the cluster stack on the node where the upgrade has been completed, allowing the node to rejoin the cluster.  crm cluster start Before moving to the next node, check the cluster status with the command below or HAWK.  crm_mon -rfn1 Take the next node offline and repeat the procedure for that node.   © 2020, Amazon Web Services, Inc. or its affiliates. All rights reserved.\r"
},
{
	"uri": "/en/lab3/db.html",
	"title": "Database Server",
	"tags": [],
	"description": "",
	"content": "Lab 설명 Let\u0026rsquo;s learn how to check whether SAP HANA Cluster configured in Lab01 works properly in test cases.\n  Lab Architecture In this Lab, you will test the following three cases. In each case, you are going to learn how to check whether HANA DB is normally taken over.\n Test Case 01: Stop Database(Primary \u0026lt;\u0026ndash;\u0026gt; Secondary) Test Case 02: Crash Database(Primary \u0026lt;\u0026ndash;\u0026gt; Secondary) Test Case 03: System crash(Primary \u0026lt;\u0026ndash;\u0026gt; Secondary)    Test Architecture   Secondary(on sechana) to take over as Primary   Secondary(on prihana) to fail back as Primary    © 2020, Amazon Web Services, Inc. or its affiliates. All rights reserved.\r"
},
{
	"uri": "/en/lab3.html",
	"title": "Lab 03. Test Cluster",
	"tags": [],
	"description": "",
	"content": "Lab 설명 Let\u0026rsquo;s learn how to check whether S/4HANA Cluster configured in Lab01 works properly in test cases.\n  Lab Architecture In this Lab, you will test the following three cases. In each case, you are going to learn how to check whether HANA DB is normally taken over.\n Test Case 01: Stop Database(Primary \u0026lt;\u0026ndash;\u0026gt; Secondary) Test Case 02: Crash Database(Primary \u0026lt;\u0026ndash;\u0026gt; Secondary) Test Case 03: System crash(Primary \u0026lt;\u0026ndash;\u0026gt; Secondary)    Test Architecture   Secondary(on sechana) to take over as Primary   Secondary(on prihana) to fail back as Primary    © 2020, Amazon Web Services, Inc. or its affiliates. All rights reserved.\r"
},
{
	"uri": "/en/lab3/db/lab3-1.html",
	"title": "Task 01. Verify Cluster",
	"tags": [],
	"description": "",
	"content": " Before proceeding with the HA test, let\u0026rsquo;s verify that the cluster is normally state and connect to the instance through the Session Manager.\n   Connect to prihana instance through Session Manager. After log in to AWS Management Console, Connect to EC2 Instance Console. Choose HANA-HDB-Primary instance and click Connect.  Select Session Manager and click Connect.  Check SAP HANA System Replication (HSR) status.    Check the status using hdbnsutil -sr_state command (using the hdbadm user)\n You can see that HAP(prihana) site is primary and HAS(sechana) site is sync.  sudo su - hdbadm hdbnsutil -sr_state   Check the status using HDBSettings.sh systemReplicationStatus.py command (using the hdbadm user)\n You can see that Primary node is prihana and Secondary node is sechana.  HDBSettings.sh systemReplicationStatus.py   Check the status using SAPHanaSR-showAttr command (using root user)\n You can see that sechana node is SOK  exit sudo su - SAPHanaSR-showAttr    Check the cluster configuration.\n crm is a tool to configure and manage clusters. To check how HA works, check the two options PREFER_SITE_TAKEOVER and AUTOMATED_REGISTER. If both are true, it means that Takeover is automatically performed, and if Secondary is normal, it is automatically registered. Confirm Cluster configuration using the crm configure show command (using root user)  crm configure show   Check the cluster status.\n crm_mon is a command that provides the current state of the cluster (using root user). Execute this command to check if prihana is in Master status.  crm_mon -rfn1    © 2020, Amazon Web Services, Inc. or its affiliates. All rights reserved.\r"
},
{
	"uri": "/en/lab3/db/lab3-2.html",
	"title": "Task 02. Stop Database",
	"tags": [],
	"description": "",
	"content": " Let\u0026rsquo;s learn how the cluster works when the HANA database is stopped\n  Stop Primary Database on prihana When HDB STOP occurs in primary node prihana, check whether secondary node sechana node is normally changed to the primary node.   Connect to EC2 Instance Console\n  Select HANA-HDB-Primary instance and click Connect.   Select Session Manager, click Connect and then connect to prihana instance through Session Manager.   Stop HANA DB on prihana (use hdbadm user)\nsudo su - hdbadm HDB stop   Check cluster and HSR status on sechana\n  Connect to EC2 Instance Console\n  Select HANA-HDB-Secondary instance and click Connect.   Select Session Manager, click Connect and then connect to sechana instance through Session Manager.   Check cluster status.\n crm_mon is a command that provides the current state of cluster (use root user). Execute the command below to check if sechana is in Master status.  sudo su - crm_mon -rfn1   Check SAP HANA System Replication (HSR)\n Using SAPHanaSR-showAttr, prihana node confirms SOK (use root user)  SAPHanaSR-showAttr   After taking over, clean up the resource status\ncrm resource cleanup rsc_SAPHanaTopology_HDB_HDB00   Connect to Bastion Host and check the Dashboard of HAWK web (refer to Lab02)\n You can see that Overlay IP Resource has been moved to sechana.      Stop Primary Database on sechana When HDB STOP occurs in primary node sechana, check whether secondary node prihana node is normally changed to primary node.   Connect to EC2 Instance Console\n  Select HANA-HDB-Secondary instance and click Connect.   Select Session Manager, click Connect and then connect to sechana instance through Session Manager.   Stop HANA DB on sechana (use hdbadm user)\nsudo su - hdbadm HDB stop   Check cluster and HSR status on prihana.\n  Connect to EC2 Instance Console\n  Select HANA-HDB-Primary instance and click Connect.   Select Session Manager, click Connect and then connect to prihana instance through Session Manager.   Check cluster status.\n crm_mon is a command that provides the current state of cluster (use root user). Execute the command below to check if prihana is in Master status.  sudo su - crm_mon -rfn1   Check SAP HANA System Replication (HSR)\n Using SAPHanaSR-showAttr, sechana node confirms SOK (use root user)  SAPHanaSR-showAttr   After taking over, clean up the resource status\ncrm resource cleanup rsc_SAPHanaTopology_HDB_HDB00   Connect to Bastion Host and check the Dashboard of HAWK web (refer to Lab02)\n You can see that Overlay IP Resource has been moved to prihana.      © 2020, Amazon Web Services, Inc. or its affiliates. All rights reserved.\r"
},
{
	"uri": "/en/lab3/db/lab3-3.html",
	"title": "Task 03. Crash Database",
	"tags": [],
	"description": "",
	"content": " Let\u0026rsquo;s learn how the cluster works when DB crash.\n  Crash Primary Database on prihana When DB Crash occurs in primary node prihana, check whether secondary node sechana node is normally changed to primary node.   Connect to EC2 Instance Console\n  Select HANA-HDB-Primary instance and click Connect.   Select Session Manager, click Connect and then connect to prihana instance through Session Manager.   Execute HDB kill on prihana (use hdbadm user)\nsudo su - hdbadm HDB kill -9   Check cluster and HSR status on sechana\n  Connect to EC2 Instance Console\n  Select HANA-HDB-Secondary instance and click Connect.   Select Session Manager, click Connect and then connect to sechana instance through Session Manager.   Check cluster status.\n crm_mon is a command that provides the current state of cluster (use root user). Execute the command below to check if sechana is in Master status  sudo su - crm_mon -rfn1   Check SAP HANA System Replication (HSR)\n Using SAPHanaSR-showAttr, prihana node confirms SOK (use root user)  SAPHanaSR-showAttr   After taking over, clean up the resource status\ncrm resource cleanup rsc_SAPHanaTopology_HDB_HDB00   Connect to Bastion Host and check the Dashboard of HAWK web (refer to Lab02)\n You can see that Overlay IP Resource has been moved to sechana.      Crash Primary Database on sechana When DB Crash occurs in primary node sechana, check whether secondary node prihana node is normally changed to the primary node.   Connect to EC2 Instance Console\n  Select HANA-HDB-Secondary instance and click Connect.   Select Session Manager, click Connect and then connect to sechana instance through Session Manager.   Execute HDB kill on sechana (use hdbadm user)\nsudo su - hdbadm HDB kill -9   Check cluster and HSR status on prihana.\n  Connect to EC2 Instance Console\n  Select HANA-HDB-Primary instance and click Connect.   Select Session Manager, click Connect and then connect to prihana instance through Session Manager.   Check cluster status.\n crm_mon is a command that provides the current state of cluster (use root user). Execute the command below to check if prihana is in Master status.  sudo su - crm_mon -rfn1   Check SAP HANA System Replication (HSR)\n Using SAPHanaSR-showAttr, sechana node confirms SOK (use root user)  SAPHanaSR-showAttr   After taking over, clean up the resource status\ncrm resource cleanup rsc_SAPHanaTopology_HDB_HDB00   Connect to Bastion Host and check the Dashboard of HAWK web (refer to Lab02)\n You can see that Overlay IP Resource has been moved to prihana.      © 2020, Amazon Web Services, Inc. or its affiliates. All rights reserved.\r"
},
{
	"uri": "/en/lab3/db/lab3-4.html",
	"title": "Task 04. Crash System",
	"tags": [],
	"description": "",
	"content": " Let\u0026rsquo;s learn how the cluster works when OS crash.\n  Crash Primary Site Node(prihana) When System Crash occurs in primary node prihana, check whether secondary node sechana node is normally changed to primary node.   Connect to EC2 Instance Console\n  Select HANA-HDB-Primary instance and click Connect.   Select Session Manager, click Connect and then connect to prihana instance through Session Manager.   Execute fast-reboot on prihana (use root user)\nsudo su - echo \u0026#39;b\u0026#39; \u0026gt; /proc/sysrq-trigger   Check cluster and HSR status on sechana\n  Connect to EC2 Instance Console\n  Select HANA-HDB-Secondary instance and click Connect.   Select Session Manager, click Connect and then connect to sechana instance through Session Manager.   Check cluster status.\n crm_mon is a command that provides the current state of cluster (use root user). Execute the command below to check if sechana is in Master status. Unlike the previous case, prihana is in OFFLINE status.  sudo su - crm_mon -rfn1   Check SAP HANA System Replication (HSR)\n As a result of executing the SAPHanaSR-showAttr command, the prihana node is in offline status (use root user).  SAPHanaSR-showAttr   After taking over, clean up the resource status\ncrm resource cleanup rsc_SAPHanaTopology_HDB_HDB00   Connect to Bastion Host and check the Dashboard of HAWK web (refer to Lab02)\n Unlike the previous case, prihana is offline.     Connect to EC2 Instance Console\n  HANA-HDB-Primary instance is in stopped state. Start prihana to check if it is in a normal state.   Select HANA-HDB-Primary instance, and click Action. Press Start in Instance State. Press Yes, Start.   If HANA-HDB-Primary boots normally, connect to sechana instance. check cluster and HSR status.\n  Check cluster status.\n Execute the command below to check if prihana is in Slave status (use root user).  sudo su - crm_mon -rfn1   Check SAP HANA System Replication (HSR)\n Using SAPHanaSR-showAttr, prihana node confirms SOK (use root user)  SAPHanaSR-showAttr   clean up the resource status (use root user)\ncrm resource cleanup rsc_SAPHanaTopology_HDB_HDB00   Connect to Bastion Host and check the Dashboard of HAWK web (refer to Lab02)\n You can see that prihana has changed to online status.      Crash Secondary Site Node (sechana) When System Crash occurs in primary node sechana, check whether secondary node prihana node is normally changed to primary node.   Connect to EC2 Instance Console\n  Select HANA-HDB-Secondary instance and click Connect.   Select Session Manager, click Connect and then connect to sechana instance through Session Manager.   Execute fast-reboot on sechana (use root user)\nsudo su - echo \u0026#39;b\u0026#39; \u0026gt; /proc/sysrq-trigger   Check cluster and HSR status on prihana.\n  Connect to EC2 Instance Console\n  Select HANA-HDB-Primary instance and click Connect.   Select Session Manager, click Connect and then connect to prihana instance through Session Manager.   Check cluster status.\n crm_mon is a command that provides the current state of cluster (use root user). Execute the command below to check if prihana is in Master status. Unlike the previous case, sechana is in OFFLINE status.  sudo su - crm_mon -rfn1   Check SAP HANA System Replication (HSR)\n As a result of executing the SAPHanaSR-showAttr command, the sechana node is in offline status (use root user).  SAPHanaSR-showAttr   After taking over, clean up the resource status\ncrm resource cleanup rsc_SAPHanaTopology_HDB_HDB00   Connect to Bastion Host and check the Dashboard of HAWK web (refer to Lab02))\n Unlike the previous case, sechana is offline.     Connect to EC2 Instance Console\n  HANA-HDB-Secondary instance is in stopped state. Start sechana to check if it is in a normal state.   Select HANA-HDB-Secondary instance, and click Action. Press Start in Instance State. Press Yes, Start.   If HANA-HDB-Secondary boots normally, connect to sechana instance. check cluster and HSR status.\n  Check cluster status.\n Execute the command below to check if sechana is in Slave status (use root user).  sudo su - crm_mon -rfn1   Check SAP HANA System Replication (HSR)\n Using SAPHanaSR-showAttr, sechana node confirms SOK (use root user)  SAPHanaSR-showAttr   clean up the resource status (use root user)\ncrm resource cleanup rsc_SAPHanaTopology_HDB_HDB00   Connect to Bastion Host and check the Dashboard of HAWK web (refer to Lab02)\n You can see that sechana has changed to online status      © 2020, Amazon Web Services, Inc. or its affiliates. All rights reserved.\r"
},
{
	"uri": "/en/lab3/ap.html",
	"title": "Application Server",
	"tags": [],
	"description": "",
	"content": "Lab 설명 Let\u0026rsquo;s learn how to check whether SAP HANA Cluster configured in Lab01 works properly in test cases.\n  Lab Architecture In this Lab, you will test the following three cases. In each case, you are going to learn how to check whether ASCS/ERS is normally taken over.\n Test Case 01: Stop Server(Primary \u0026lt;\u0026ndash;\u0026gt; Secondary) Test Case 02: Crash Server(Primary \u0026lt;\u0026ndash;\u0026gt; Secondary) Test Case 03: System crash(Primary \u0026lt;\u0026ndash;\u0026gt; Secondary)    Test Architecture   Secondary(on s4hers) to take over as Primary(ASCS/ERS)   Secondary(on s4hascs) to fail back as Primary(ASCS/ERS)    © 2021 Amazon Web Services, Inc. 또는 자회사, All rights reserved.\r"
},
{
	"uri": "/en/lab4.html",
	"title": "Lab 04. Admin Cluster",
	"tags": [],
	"description": "",
	"content": "Lab 설명 Let\u0026rsquo;s take a look at the various HA Cluster Commands required during HA Cluster operation.\n  Lab Architecture In this lab, we will run the HA cluster commands required for various situations while operating the cluster..\n Perform HANA migration using cluster migration command Taking the node offline by putting the cluster node in standby. Execute SAP HANA System Replication Test Cluster Resource Agents SUSE Cluster Software Upgrading Concept and Procedure    Test Architecture   Secondary(on sechana) to take over as Primary   Secondary(on prihana) to take back as Primary    © 2020, Amazon Web Services, Inc. or its affiliates. All rights reserved.\r"
},
{
	"uri": "/en/lab5.html",
	"title": "Lab 05. Change to Cost Optimized Scenario",
	"tags": [],
	"description": "",
	"content": "Lab Introduction By changing the HA setting of SAP HANA DB of Performance Optimized Scenario installed with QuickStart, we plan to configure Cost Optimized Scenario with Standby and QAS system. Please refer to the link below for details on setting\n  SAP HANA High Availability Cluster for the AWS Cloud - Setup Guide   Lab Architecture  © 2020, Amazon Web Services, Inc. or its affiliates. All rights reserved.\r"
},
{
	"uri": "/en/lab5/lab5-1.html",
	"title": "Task 01. Change HANA HSR Config",
	"tags": [],
	"description": "",
	"content": " Changing the HSR option for QAS system on standby system. We are going to change Global Memory setting.\n    Connect to sechana through Session Manager.\n After log in AWS Management Console, connect to EC2 Instance Console Select HANA-HDB-Secondary instance, click Action, and click Connect.  Select Session Manager and click Connect.     After connecting as root user, change cluster to maintenance mode and change HSR and Global Memory settings.\n Change cluster to maintenance mode.  sudo su - root crm node maintenance prihana crm node maintenance sechana  Check crm maintenance status  crm_mon -rfn1   After connecting as hdbadm user, stop HDB, reduce memory resource usage of the secondary SAP HANA DB (sechana), and set preload option to false.\n (sechana) HDB STOP  su - hdbadm HDB stop  (sechana) Change global.ini settings. Open global.ini with VI editor. And add the options below.  vi /usr/sap/HDB/SYS/global/hdb/custom/config/global.ini [system_replication] ... preload_column_tables = false #Add-on [memorymanager] global_allocation_limit = 24576   After Takover, change SAPHanaSR-Hook so that the existing settings can be restored.\n (sechana) After log in root user, Back up the script before modifying SAPHanaSR.py.  exit cd /usr/share/SAPHanaSR/ cp -pr SAPHanaSR.py SAPHanaSR.py.default  (sechana) Modify the SAPHanaSR.py script as follows:  Block 1(AS-Is)  Block 1(To-Be)  Block 2(AS-Is)  Block 2(To-Be)  Please modify the script as below.  vi SAPHanaSR.py # Block 1 from hdb_ha_dr.client import HADRBase, Helper import os, time from hdbcli import dbapi dbuser = \u0026#34;SYSTEM\u0026#34; dbpwd = \u0026#34;Init12345!\u0026#34; dbport = 30013 stmnt1 = \u0026#34;ALTER SYSTEM ALTER CONFIGURATION (\u0026#39;global.ini\u0026#39;,\u0026#39;SYSTEM\u0026#39;) UNSET (\u0026#39;memorymanager\u0026#39;,\u0026#39;global_allocation_limit\u0026#39;) WITH RECONFIGURE\u0026#34; stmnt2 = \u0026#34;ALTER SYSTEM ALTER CONFIGURATION (\u0026#39;global.ini\u0026#39;,\u0026#39;SYSTEM\u0026#39;) UNSET (\u0026#39;system_replication\u0026#39;,\u0026#39;preload_column_tables\u0026#39;) WITH RECONFIGURE\u0026#34; # Block 2 def postTakeover(self, rc, **kwargs): \u0026#34;\u0026#34;\u0026#34;Post takeover hook.\u0026#34;\u0026#34;\u0026#34; self.tracer.info(\u0026#34;%s.postTakeover method called with rc=%s\u0026#34; % (self.__class__.__name__, rc)) if rc == 0: # normal takeover succeeded conn = dbapi.connect(\u0026#39;localhost\u0026#39;, dbport, dbuser, dbpwd) cursor = conn.cursor() cursor.execute(stmnt1) cursor.execute(stmnt2) return 0 elif rc == 1: # waiting for force takeover conn = dbapi.connect(\u0026#39;localhost\u0026#39;, dbport, dbuser, dbpwd) cursor = conn.cursor() cursor.execute(stmnt1) cursor.execute(stmnt2) return 0 elif rc == 2: # error, something went wrong return 0     After connecting as hdbadm user, start HDB, and perform TAKEOVER TEST.\n (sechana) HDB START  su - hdbadm HDB start  (sechana) Check HSR status.  hdbnsutil -sr_state  (sechana) takeover to sechana  hdbnsutil -sr_takeover  (sechana) After Takeover, check whether global.ini setting is restored by SAPHanaSR.py.  cat /usr/sap/HDB/SYS/global/hdb/custom/config/global.ini  Since HA Cluster is currently in maintenance mode, automatic Takeover or Takeback using HA Cluster is not possible. To restore HSR to its original state, manually reset the HSR in the following order. Keep the connection of sechana, and also connect to prihana through Session Manager.  Connect to EC2 Instance Console. Select HANA-HDB-Primary instance, click Action, and click Connect.  Select Session Manager and click Connect.    (prihana) After connecting as hdbadm user, execute sr_register as seconadry Database  sudo su - hdbadm HDB stop hdbnsutil -sr_register --remoteHost=sechana --remoteInstance=00 --replicationMode=sync --name=HAP --operationMode=logreplay HDB start  (prihana) Check the synchronization status  hdbnsutil -sr_state  (prihana) takeback to prihana  hdbnsutil -sr_takeover  (sechana) After connecting as hdbadm user, execute sr_register as seconadry Database  su - hdbadm HDB stop hdbnsutil -sr_register --remoteHost=prihana --remoteInstance=00 --replicationMode=sync --name=HAS --operationMode=logreplay  (sechana) Change global.ini settings. Open global.ini with VI editor. And add the options below.  vi /usr/sap/HDB/SYS/global/hdb/custom/config/global.ini [system_replication] ... preload_column_tables = false #Add-on [memorymanager] global_allocation_limit = 24576  (sechana) After connecting as hdbadm user, HDB start  HDB start    © 2020, Amazon Web Services, Inc. or its affiliates. All rights reserved.\r"
},
{
	"uri": "/en/lab5/lab5-2.html",
	"title": "Task 02. Install SAP HANA DATABASE (QAS)",
	"tags": [],
	"description": "",
	"content": " Let\u0026rsquo;s install QAS(Non-productive) SAP HANA database in sechana. In this Lab, we plan to install only for minimum functional verification. For detailed information related to the installation of HANA DB, please refer to the SAP HANA Administration Guide below.\n  SAP HANA Administration Guide     Connect to sechana through Session Manager.\n Connect to EC2 Instance Console Select HANA-HDB-Secondary instance, click Action, and click Connect.  Select Session Manager and click Connect.     Install awscli to download the HANA installation file from S3.\n Please refer to the next link for detailed installation instructions. Install the AWS CLI version 2 on Linux After connecting as root user, install the latest version of the AWS CLI as follows.  sudo su - cd /hana/shared curl \u0026#34;https://awscli.amazonaws.com/awscli-exe-linux-x86_64.zip\u0026#34; -o \u0026#34;awscliv2.zip\u0026#34; unzip awscliv2.zip ./aws/install  Confirm awscli installation as below  aws --version   After downloading HANA installation file as follows, extract it.\n Download HANA installation file from S3 URL shared in the lab guide.(e.g s3://sap-immsersionday-hol1/hanadb)  mkdir -p /hana/shared/hanadb cd /hana/shared/hanadb aws s3 cp s3://sap-immsersionday-hol1/hanadb . --recursive  Unrar HANA installation file as follows.  unrar x 51053381_part1.exe   Install HANA DB\n  The main installation options are listed below.\n System ID : QAS Instance Number : 10 Master Password : Init12345! Restrict maximum memory allocation : y Enter Maximum Memory Allocation in MB [63615]: 32768    Install HANA DB using hdblcm.\n  cd /hana/shared/hanadb/51053381/DATA_UNITS/HDB_LCM_LINUX_X86_64 ./hdblcm   Enter the installation options as below.\n choose an action[install] : 2  Select additional components for installation[server] : 2  Use default settings for Install Path and Host Name, and enter QAS for System ID and 10 for Instance Number.  Select 2 for System usage as test, enter 32769 MB for memory setting and Init12345! for password, and use Default setting for the rest.  Do you want to continue? (y/n) y     When the installation is complete, you can see the results below.     Check QAS HANA DB installation.\n Switch to qasadm user and check if HDB instance is running normally.  su - qasadm HDB info    © 2020, Amazon Web Services, Inc. or its affiliates. All rights reserved.\r"
},
{
	"uri": "/en/lab5/lab5-3.html",
	"title": "Task 03. Change Config of the Cluster for QAS system",
	"tags": [],
	"description": "",
	"content": " We will change the existing cluster configuration to manage PRD HANA Database and newly added QAS HANA Database.\n  To change Cost Optimized scenario, it is necessary to change PREFER_SITE_TAKEOVER option for HANA DB resources of the cluster as follows. For more information, please refer to the link below.  SAP HANA High Availability Cluster for the AWS Cloud - Setup Guide        For changing cluster setup, stop all DB of sechana and prihana node.\n  Connect to sechana through Session Manager.\n  After log in AWS Management Console, connect to EC2 Instance Console\n  Select HANA-HDB-Secondary instance, click Action, and click Connect.   Select Session Manager and click Connect.   Stop QAS DB.\n  sudo su - qasadm HDB stop exit  Stop PRD(HDB) DB.  sudo su - hdbadm HDB stop exit   Keep the connection of sechana, Connect to prihana through Session Manager.\n After log in AWS Management Console, connect to EC2 Instance Console Select HANA-HDB-Primary instance, click Action, and click Connect.  Select Session Manager and click Connect.  Stop PRD(HDB) DB.  sudo su - hdbadm HDB stop exit     After switching to root user in prihana node, change the PREFER_SITE_TAKEOVER option for PRD(HDB) DB resource in the cluster from true to false.\n (root) Create a folder for snippet creation  sudo su - mkdir -p /hana/shared/script/ cd /hana/shared/script/  (root) Using VI editor, create a snippet as shown below.  vi crm-SAPHana-update.txt primitive rsc_SAPHana_HDB_HDB00 ocf:suse:SAPHana \\  operations $id=rsc_sap_HDB_HDB00-operations \\  op start interval=0 timeout=3600 \\  op stop interval=0 timeout=3600 \\  op promote interval=0 timeout=3600 \\  op monitor interval=60 role=Master timeout=700 \\  op monitor interval=61 role=Slave timeout=700 \\  params SID=HDB InstanceNumber=00 PREFER_SITE_TAKEOVER=false DUPLICATE_PRIMARY_TIMEOUT=7200 AUTOMATED_REGISTER=true  (root) Load the snippet to crm.  crm configure load update crm-SAPHana-update.txt  (root) Check the updated crm settings.  crm config show   On prihana node, register the resource for QAS DB in the cluster.\n (root) Using VI editor, create a snippet as shown below.  vi crm-qas.txt primitive rsc_SAP_QAS_HDB10 ocf:heartbeat:SAPDatabase \\  params DBTYPE=\u0026#34;HDB\u0026#34; SID=\u0026#34;QAS\u0026#34; InstanceNumber=\u0026#34;10\u0026#34; \\  MONITOR_SERVICES=\u0026#34;hdbindexserver|hdbnameserver\u0026#34; \\  op start interval=\u0026#34;0\u0026#34; timeout=\u0026#34;600\u0026#34; \\  op monitor interval=\u0026#34;120\u0026#34; timeout=\u0026#34;700\u0026#34; \\  op stop interval=\u0026#34;0\u0026#34; timeout=\u0026#34;300\u0026#34; \\  meta priority=\u0026#34;100\u0026#34;  (root) Load the snippet to crm.  crm configure load update crm-qas.txt   On prihana node, Set the newly created QAS HANA DB resource constraints\n (root) Using VI editor, create a snippet as shown below.  vi crm-cs-qas.txt location loc_QAS_never_on_prihana rsc_SAP_QAS_HDB10 -inf: prihana colocation col_QAS_never_with_AWS_IP -inf: rsc_SAP_QAS_HDB10:Started \\ res_AWS_IP order ord_QASstop_before_HDB-promote inf: rsc_SAP_QAS_HDB10:stop \\ msl_SAPHana_HDB_HDB00:promote  (root) Load the snippet to crm.  crm configure load update crm-cs-qas.txt  (root) Check the updated crm settings.  crm config show   On prihana node, turn off Cluster Maintenance mode and monitor whether it is properly set.\n Turn off Cluster Maintenance mode  crm node ready prihana crm node ready sechana  Monitor the cluster status. Check if the QAS HANA DB resource is properly Started.  crm_mon -rfn1  Monitor HSR status. Check that sechana\u0026rsquo;s status is SOK.  SAPHanaSR-showAttr   After switching to qasadm user in sechana node, check if QAS HANA DB is started normally.\n Check if HDB processes are started normally.  sudo su - qasadm HDB info    © 2020, Amazon Web Services, Inc. or its affiliates. All rights reserved.\r"
},
{
	"uri": "/en/lab5/lab5-4.html",
	"title": "Task 04. Test Cluster",
	"tags": [],
	"description": "",
	"content": " Test the changed cluster to verify if it operates normally according to the scenario.\n   The first scenario is to check how the cluster works when prihana\u0026rsquo;s DB crashes.    Connect to prihana through Session Manager.\n  After log in AWS Management Console, connect to EC2 Instance Console\n  Select HANA-HDB-Primary instance, click Action, and click Connect.   Select Session Manager and click Connect.   After switching to hdbadm user in prihana node, Crash(HDB kill) HANA DB.\n  sudo su - hdbadm HDB kill -9 exit  After switching to root user in prihana node, monitor the cluster status.  sudo su - crm_mon -rfn1  Monitor HSR status. Check the status of sechana node is SOK.  SAPHanaSR-showAttr  Since prihana instance is in normal state and PREFER_SITE_TAKEOVER option has been changed to false, DB was restarted in prihana without being taken over to sechana.     The second scenario checks how the cluster works when Stop prihana Instance.\n  Crash OS(Stop Instance)\n After log in AWS Management Console, connect to EC2 Instance Console Select HANA-HDB-Primary instance, click Instance state, and click Stop Instance.     Connect to sechana through Session Manager.\n After log in AWS Management Console, connect to EC2 Instance Console Select HANA-HDB-Secondary instance, click Action, and click Connect.  Select Session Manager and click Connect.     Monitor the cluster status\n  sudo su - crm_mon -rfn  Since prihana node was stopped, it was normally taken over by sechana. QAS system was shut down for normal service, and Memory limit setting has been released so that memory can be serviced normally.  sudo su - hdbadm cat /usr/sap/HDB/SYS/global/hdb/custom/config/global.ini   Start prihana node. If prihana node is normall, take back to prihana to use QAS system. Restore sechana\u0026rsquo;s PRD(HDB)\u0026rsquo;s global.ini settings.\n  Start prihana instance.\n After log in AWS Management Console, connect to EC2 Instance Console. Select HANA-HDB-Primary instance, click Action, Select Start Instance from Instance state.     Switch to root user in sechana node. After prihana instance starts up normally, check if HDB HANA resource is Slave.\n  sudo su - crm_mon -rfn  On sechana node, Check HSR status to verify if prihana status is SOK.  SAPHanaSR-showAttr  On sechana node, take back to prihana. Change node to standby to move all resources to prihana.  crm node standby sechana  Verify prihana\u0026rsquo;s HDB HANA resource has been switched to Master.  crm_mon -rfn  After switching to hdbadm user in sechana node, restore global.ini using VI editor.  sudo su - hdbadm vi /usr/sap/HDB/SYS/global/hdb/custom/config/global.ini [system_replication] ... preload_column_tables = false #Add-on [memorymanager] global_allocation_limit = 24576  After switching to root user in sechana node, make node online.  sudo su - crm node online sechana  Verify sechana\u0026rsquo;s HDB HANA resource is changed Slave and QAS HANA resource is Started  crm_mon -rfn1  Check the HSR status to verify if sechana status is SOK.  SAPHanaSR-showAttr    © 2020, Amazon Web Services, Inc. or its affiliates. All rights reserved.\r"
},
{
	"uri": "/en/lab3/ap/lab3-ap-1.html",
	"title": "Task 01. Verify Cluster",
	"tags": [],
	"description": "",
	"content": "Before proceeding with the HA test, let\u0026rsquo;s verify that the cluster is normally state and connect to the instance through the Session Manager.\n Connect to the EC2 instance named ~ ASCS : s4hascs through Session Manager, following the same steps you did in the previous lab; from the EC2 Instance page in the AWS Management Console.    First, we want to check the cluster configuration. crm configure show will show the current objects in the Cluster Information Database (CIB), which allows you to check the cluster configuration. The crm tool manages objects in your cluster, comes with the High Availability Extension (HAE) in SLES for SAP Applications.\ncrm configure show   You can check the current cluster status through crm_mon. It shows each of the resources in your cluster, and their current status. Run this command with the following arguments, to check if s4hascs is in S4H_ASCS10 Started and s4hers is in S4H_ERS11 Started . crm_mon also comes with HAE.\ncrm_mon -rfn1   Check if the name of the SUSE cluster solution is shown in the output of sapcontrol or SAP management console. This test checks the status of the SAP NetWeaver cluster integration.\n sapcontrol -nr \u0026lt;instance number\u0026gt; -function HAGetFailoverConfig  ASCS : 10 ERS : 11    su - s4hadm sapcontrol -nr 11 -function HAGetFailoverConfig    © 2019 Amazon Web Services, Inc. 또는 자회사, All rights reserved.\r"
},
{
	"uri": "/en/lab3/ap/lab3-ap-2.html",
	"title": "Task 02. Migrate ASCS",
	"tags": [],
	"description": "",
	"content": "Let\u0026rsquo;s learn how the cluster reacts when the SAP Application server is stopped.\n2.1 Migrate ASCS to s4hers When the below command occurs in s4hascs, check whether s4hers node is normally changed to ASCS Active node.  Use Session Manager to connect to the ~ ASCS : s4hascs instance.  You will only need to re-open your session if you have already disconnected from it. We recommend reusing the same session with this instance. Leave your session open and go back to it when needed, as we will need to access it several times, for both prihana and sechana.\n   Migrate the ASCS on this instance (s4hascs) using the \u0026lt;sid\u0026gt;adm user, which should be s4hadm for you, on both nodes. Check if moving the ASCS using SAP tools like sapcontrol does work properly. after migrating, automatly migrate ERS to s4hascs\nsudo su - s4hadm sapcontrol -nr 10 -function HAFailoverToNode \u0026#34;\u0026#34;   Let\u0026rsquo;s see how this affects s4hers node. In the following steps, we will connect to the ~ ERS : s4hers instance using Session Manager, and check the cluster\n  Similar to s4hascs, leave this session to s4hers open and reuse it.\n   Go back to the list of EC2 Instances in your AWS Management Console. Select ~ ERS : s4hers instance and click Connect.\n  Select Session Manager, click Connect and then connect to sechana instance through Session Manager.\n  Check cluster status. Use sapcontrol to check if s4hers is in Active status. If it does not show Active status yet, wait and try again.\nsu - s4hadm sapcontrol -nr 10 -function HAGetFailoverConfig   Check cluster status. Use crm_mon to check if ERS is automatly migrated to s4hers. If it does not show Active status yet, wait and try again.\nsudo su - crm_mon -rfn1    2.2 Migrate ASCS to s4hascs When the below command occurs in s4hers, check whether s4hascs node is normally changed to ASCS Active node.   Use Session Manager to connect to the ~ ERS : s4hers instance.\n  Migrate the ASCS on this instance (s4hers) using the \u0026lt;sid\u0026gt;adm user, which should be s4hadm for you, on both nodes. Check if moving the ASCS using SAP tools like sapcontrol does work properly. after migrating, automatly migrate ERS to s4hers\nsudo su - s4hadm sapcontrol -nr 10 -function HAFailoverToNode \u0026#34;\u0026#34;   Now, let\u0026rsquo;s go back and check the cluster on s4hascs node.\n  Use Session Manager to connect to ~ ASCS : s4hascs (s4hascs).\n  Check cluster status. Use sapcontrol to check if s4hascs is in Active status. If it does not show Active status yet, wait and try again.\nsu - s4hadm sapcontrol -nr 10 -function HAGetFailoverConfig   Check cluster status. Use crm_mon to check if ERS is automatly migrated to s4hers. If it does not show Active status yet, wait and try again.\nsudo su - crm_mon -rfn1    © 2019 Amazon Web Services, Inc. 또는 자회사, All rights reserved.\r"
},
{
	"uri": "/en/lab3/ap/lab3-ap-3.html",
	"title": "Task 03. Crash System",
	"tags": [],
	"description": "",
	"content": "Let\u0026rsquo;s learn how the cluster reacts when the SAP Application server is crashed.\n2.1 Crashed ASCS on s4hers When a system crash occurs in s4hascs, check whether s4hers node is normally changed to ASCS Active node.  Use Session Manager to connect to the ~ ASCS : s4hascs instance.  You will only need to re-open your session if you have already disconnected from it. We recommend reusing the same session with this instance. Leave your session open and go back to it when needed, as we will need to access it several times, for both prihana and sechana.\n   Use Session Manager to connect to ~ ASCS : s4hascs (s4hascs).\n  To simulate a system crash, we will do a forced stop of the operating system, via a fast-shutdown on s4hascs (use root user). Your Session Manager session will become unresponsive after this, you can close that tab/window in your browser.\nsudo su - echo b \u0026gt;/proc/sysrq-trigger   Since s4hascs has now crashed, let\u0026rsquo;s check cluster on s4hers. Use Session Manager to connect to ~ ERS : s4hers instance.\n  Check cluster status. Use sapcontrol to check if s4hers is in Active status. If it does not show Active status yet, wait and try again.\nsu - s4hadm sapcontrol -nr 10 -function HAGetFailoverConfig   check cluster status via crm_mon (as root user). Execute the command below to check if s4hers is in Active status. Unlike the previous case, s4hascs is in OFFLINE status.\nsudo su - crm_mon -rfn1   Due to the earlier \u0026ldquo;crash\u0026rdquo;, ~ ASCS : s4hascs (s4hascs). instance is in a stopped state. Now we will start s4hascs. Go back to the list of EC2 instances in the AWS Management Console, and select ~ ASCS : s4hascs (s4hascs). Click on Instance state, and then Start instance.\n  Once ~ ASCS : s4hascs (s4hascs) is running and all checks are passed, use Session Manager to connect to ~ ERS : s4hers (s4hers).\n  In s4hers, check the cluster status of s4hascs. Execute the command below to check if s4hascs is in Active status for ERS (use root user). after s4hascs is running, automatly migrate ERS to s4hascs\nsudo su - crm_mon -rfn1    2.2 Crashed ASCS on s4hers When a system crash occurs in s4hers, check whether s4hascs node is normally changed to ASCS Active node.   Use Session Manager to connect to the ~ ERS : s4hers instance.\n  Execute a fast-shutdown on the s4hers instance (using root user). Your Session Manager session will become unresponsive after this, you can close that tab/window in your browser.\nsudo su - echo b \u0026gt;/proc/sysrq-trigger   Since s4hascs has now crashed, let\u0026rsquo;s check cluster on s4hers. Use Session Manager to connect to ~ ERS : s4hers instance.\n  Check cluster status. Use sapcontrol to check if s4hers is in Active status. If it does not show Active status yet, wait and try again.\nsu - s4hadm sapcontrol -nr 10 -function HAGetFailoverConfig   check cluster status via crm_mon (as root user). Execute the command below to check if s4hers is in Active status. Unlike the previous case, s4hascs is in OFFLINE status.\nsudo su - crm_mon -rfn1   Due to the \u0026ldquo;crash\u0026rdquo;, ~ ERS : s4hers (s4hers). instance is in a stopped state. Now we will start s4hers. Go back to the list of EC2 instances in the AWS Management Console, and select ~ ERS : s4hers (s4hers). Click on Instance state, and then Start instance.\n  Once ~ ERS : s4hers (s4hers) is running and all checks are passed, use Session Manager to connect to ~ ASCS : s4hascs (s4hascs).\n  In s4hascs, check the cluster status of s4hers. Execute the command below to check if s4hers is in Active status for ERS (use root user). after s4hers is running, automatly migrate ERS to s4hers\nsudo su - crm_mon -rfn1    © 2019 Amazon Web Services, Inc. 또는 자회사, All rights reserved.\r"
},
{
	"uri": "/en/categories.html",
	"title": "Categories",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "/en/tags.html",
	"title": "Tags",
	"tags": [],
	"description": "",
	"content": ""
}]